{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "224a6048",
   "metadata": {},
   "source": [
    "# Importar Dependecias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "757f76a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 01:11:26.484564: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-06-09 01:11:26.484598: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow_docs.vis import embed\n",
    "from tensorflow import keras\n",
    "from imutils import paths\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import imageio\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca365d7a",
   "metadata": {},
   "source": [
    "Hiperparametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af35e326",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 20\n",
    "\n",
    "MAX_SEQ_LENGTH = 900\n",
    "NUM_FEATURES = 2048"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888279d9",
   "metadata": {},
   "source": [
    "# Cargando datos de train y test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "684db3a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total videos for training: 48\n",
      "Total videos for testing: 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "print(f\"Total videos for training: {len(train_df)}\")\n",
    "print(f\"Total videos for testing: {len(test_df)}\")\n",
    "type(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c2b1da",
   "metadata": {},
   "source": [
    "# Funciones para preprocesamiento del video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef19b2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_center_square(frame):\n",
    "    y, x = frame.shape[0:2]\n",
    "    min_dim = min(y, x)\n",
    "    start_x = (x // 2) - (min_dim // 2)\n",
    "    start_y = (y // 2) - (min_dim // 2)\n",
    "    return frame[start_y : start_y + min_dim, start_x : start_x + min_dim]\n",
    "\n",
    "\n",
    "def load_video(path, max_frames=0, resize=(IMG_SIZE, IMG_SIZE)):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    frames = []\n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = crop_center_square(frame)\n",
    "            frame = cv2.resize(frame, resize)\n",
    "            frame = frame[:, :, [2, 1, 0]]\n",
    "            frames.append(frame)\n",
    "\n",
    "            if len(frames) == max_frames:\n",
    "                break\n",
    "    finally:\n",
    "        cap.release()\n",
    "    return np.array(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e929703f",
   "metadata": {},
   "source": [
    "# Función para extracción de carateristicas con Inception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa712210",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 01:12:21.476081: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-09 01:12:21.477340: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/stiven/python/python/lib/python3.8/site-packages/cv2/../../lib64:\n",
      "2022-06-09 01:12:21.477517: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/stiven/python/python/lib/python3.8/site-packages/cv2/../../lib64:\n",
      "2022-06-09 01:12:21.477689: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/stiven/python/python/lib/python3.8/site-packages/cv2/../../lib64:\n",
      "2022-06-09 01:12:21.477843: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/stiven/python/python/lib/python3.8/site-packages/cv2/../../lib64:\n",
      "2022-06-09 01:12:21.477959: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/stiven/python/python/lib/python3.8/site-packages/cv2/../../lib64:\n",
      "2022-06-09 01:12:21.478069: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/stiven/python/python/lib/python3.8/site-packages/cv2/../../lib64:\n",
      "2022-06-09 01:12:21.478180: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/stiven/python/python/lib/python3.8/site-packages/cv2/../../lib64:\n",
      "2022-06-09 01:12:21.478295: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/stiven/python/python/lib/python3.8/site-packages/cv2/../../lib64:\n",
      "2022-06-09 01:12:21.478312: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-06-09 01:12:21.478777: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "def build_feature_extractor():\n",
    "    feature_extractor = keras.applications.InceptionV3(\n",
    "        weights=\"imagenet\",\n",
    "        include_top=False,\n",
    "        pooling=\"avg\",\n",
    "        input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "    )\n",
    "    preprocess_input = keras.applications.inception_v3.preprocess_input\n",
    "\n",
    "    inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))\n",
    "    preprocessed = preprocess_input(inputs)\n",
    "\n",
    "    outputs = feature_extractor(preprocessed)\n",
    "    return keras.Model(inputs, outputs, name=\"feature_extractor\")\n",
    "\n",
    "\n",
    "feature_extractor = build_feature_extractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbec4220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fight', 'normal']\n"
     ]
    }
   ],
   "source": [
    "label_processor = keras.layers.StringLookup(\n",
    "    num_oov_indices=0, vocabulary=np.unique(train_df[\"tag\"])\n",
    ")\n",
    "print(label_processor.get_vocabulary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f98544",
   "metadata": {},
   "source": [
    "# Fución para preparar lo videos\n",
    "En esta función se realiza la un preprocesamiento de los videos usando la función de extracción de caracteristicas anterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4fa9739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame features in train set: (48, 900, 2048)\n",
      "Frame masks in train set: (48, 900)\n"
     ]
    }
   ],
   "source": [
    "def prepare_all_videos(df, root_dir):\n",
    "    num_samples = len(df)\n",
    "    video_paths = df[\"video_name\"].values.tolist()\n",
    "    labels = df[\"tag\"].values\n",
    "    labels = label_processor(labels[..., None]).numpy()\n",
    "    \n",
    "    frame_masks = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH), dtype=\"bool\")\n",
    "    frame_features = np.zeros(\n",
    "        shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
    "    )\n",
    "    \n",
    "   \n",
    "    for idx, path in enumerate(video_paths):\n",
    "        frames = load_video(os.path.join(\"all_data\", path))\n",
    "        frames = frames[None, ...]\n",
    "        \n",
    "        temp_frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "        temp_frame_features = np.zeros(\n",
    "            shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
    "        )\n",
    "\n",
    "        for i, batch in enumerate(frames):\n",
    "            video_length = batch.shape[0]\n",
    "            length = min(MAX_SEQ_LENGTH, video_length)\n",
    "            for j in range(length):\n",
    "                temp_frame_features[i, j, :] = feature_extractor.predict(\n",
    "                    batch[None, j, :],\n",
    "                    verbose=0\n",
    "                )\n",
    "            temp_frame_mask[i, :length] = 1 \n",
    "\n",
    "        frame_features[idx,] = temp_frame_features.squeeze()\n",
    "        frame_masks[idx,] = temp_frame_mask.squeeze()\n",
    "\n",
    "    return (frame_features, frame_masks), labels\n",
    "\n",
    "\n",
    "train_data, train_labels = prepare_all_videos(train_df, \"train\")\n",
    "test_data, test_labels = prepare_all_videos(test_df, \"test\")\n",
    "\n",
    "print(f\"Frame features in train set: {train_data[0].shape}\")\n",
    "print(f\"Frame masks in train set: {train_data[1].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf9e54f",
   "metadata": {},
   "source": [
    "# Modelo y ejecución de un experimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87a2bf6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 02:07:07.393449: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 243302400 exceeds 10% of free system memory.\n",
      "2022-06-09 02:07:14.314036: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 235929600 exceeds 10% of free system memory.\n",
      "2022-06-09 02:07:14.314090: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 235929600 exceeds 10% of free system memory.\n",
      "2022-06-09 02:07:14.651041: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 235929600 exceeds 10% of free system memory.\n",
      "2022-06-09 02:07:14.652125: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 235929600 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - ETA: 0s - loss: 0.7256 - accuracy: 0.4848 \n",
      "Epoch 1: val_loss improved from inf to 0.61558, saving model to /tmp/video_classifier\n",
      "2/2 [==============================] - 33s 6s/step - loss: 0.7256 - accuracy: 0.4848 - val_loss: 0.6156 - val_accuracy: 0.8667\n",
      "Epoch 2/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6680 - accuracy: 0.4848\n",
      "Epoch 2: val_loss did not improve from 0.61558\n",
      "2/2 [==============================] - 3s 875ms/step - loss: 0.6680 - accuracy: 0.4848 - val_loss: 0.8260 - val_accuracy: 0.2000\n",
      "Epoch 3/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6483 - accuracy: 0.5758\n",
      "Epoch 3: val_loss did not improve from 0.61558\n",
      "2/2 [==============================] - 2s 839ms/step - loss: 0.6483 - accuracy: 0.5758 - val_loss: 0.9209 - val_accuracy: 0.2000\n",
      "Epoch 4/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.7223 - accuracy: 0.6061\n",
      "Epoch 4: val_loss did not improve from 0.61558\n",
      "2/2 [==============================] - 2s 914ms/step - loss: 0.7223 - accuracy: 0.6061 - val_loss: 0.9598 - val_accuracy: 0.2000\n",
      "Epoch 5/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.7274 - accuracy: 0.5758\n",
      "Epoch 5: val_loss did not improve from 0.61558\n",
      "2/2 [==============================] - 2s 818ms/step - loss: 0.7274 - accuracy: 0.5758 - val_loss: 0.9306 - val_accuracy: 0.2000\n",
      "Epoch 6/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.7215 - accuracy: 0.5152\n",
      "Epoch 6: val_loss did not improve from 0.61558\n",
      "2/2 [==============================] - 2s 814ms/step - loss: 0.7215 - accuracy: 0.5152 - val_loss: 0.8655 - val_accuracy: 0.2000\n",
      "Epoch 7/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6240 - accuracy: 0.5455\n",
      "Epoch 7: val_loss did not improve from 0.61558\n",
      "2/2 [==============================] - 2s 790ms/step - loss: 0.6240 - accuracy: 0.5455 - val_loss: 0.8117 - val_accuracy: 0.2000\n",
      "Epoch 8/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6557 - accuracy: 0.5152\n",
      "Epoch 8: val_loss did not improve from 0.61558\n",
      "2/2 [==============================] - 2s 800ms/step - loss: 0.6557 - accuracy: 0.5152 - val_loss: 0.7567 - val_accuracy: 0.2000\n",
      "Epoch 9/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.7116 - accuracy: 0.5455\n",
      "Epoch 9: val_loss did not improve from 0.61558\n",
      "2/2 [==============================] - 2s 838ms/step - loss: 0.7116 - accuracy: 0.5455 - val_loss: 0.7001 - val_accuracy: 0.3333\n",
      "Epoch 10/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.7188 - accuracy: 0.5152\n",
      "Epoch 10: val_loss did not improve from 0.61558\n",
      "2/2 [==============================] - 2s 789ms/step - loss: 0.7188 - accuracy: 0.5152 - val_loss: 0.6563 - val_accuracy: 0.7333\n",
      "Epoch 11/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6684 - accuracy: 0.5152\n",
      "Epoch 11: val_loss did not improve from 0.61558\n",
      "2/2 [==============================] - 3s 909ms/step - loss: 0.6684 - accuracy: 0.5152 - val_loss: 0.6267 - val_accuracy: 0.9333\n",
      "Epoch 12/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.7195 - accuracy: 0.5758\n",
      "Epoch 12: val_loss improved from 0.61558 to 0.61095, saving model to /tmp/video_classifier\n",
      "2/2 [==============================] - 2s 963ms/step - loss: 0.7195 - accuracy: 0.5758 - val_loss: 0.6109 - val_accuracy: 0.9333\n",
      "Epoch 13/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6581 - accuracy: 0.5152\n",
      "Epoch 13: val_loss did not improve from 0.61095\n",
      "2/2 [==============================] - 2s 845ms/step - loss: 0.6581 - accuracy: 0.5152 - val_loss: 0.6185 - val_accuracy: 0.8667\n",
      "Epoch 14/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6381 - accuracy: 0.6061\n",
      "Epoch 14: val_loss did not improve from 0.61095\n",
      "2/2 [==============================] - 2s 911ms/step - loss: 0.6381 - accuracy: 0.6061 - val_loss: 0.6428 - val_accuracy: 0.8667\n",
      "Epoch 15/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6026 - accuracy: 0.7576\n",
      "Epoch 15: val_loss did not improve from 0.61095\n",
      "2/2 [==============================] - 2s 985ms/step - loss: 0.6026 - accuracy: 0.7576 - val_loss: 0.6597 - val_accuracy: 0.6667\n",
      "Epoch 16/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6166 - accuracy: 0.6970\n",
      "Epoch 16: val_loss did not improve from 0.61095\n",
      "2/2 [==============================] - 2s 833ms/step - loss: 0.6166 - accuracy: 0.6970 - val_loss: 0.6761 - val_accuracy: 0.6000\n",
      "Epoch 17/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.7092 - accuracy: 0.4545\n",
      "Epoch 17: val_loss did not improve from 0.61095\n",
      "2/2 [==============================] - 2s 807ms/step - loss: 0.7092 - accuracy: 0.4545 - val_loss: 0.6987 - val_accuracy: 0.4667\n",
      "Epoch 18/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6789 - accuracy: 0.4848\n",
      "Epoch 18: val_loss did not improve from 0.61095\n",
      "2/2 [==============================] - 2s 829ms/step - loss: 0.6789 - accuracy: 0.4848 - val_loss: 0.7039 - val_accuracy: 0.4000\n",
      "Epoch 19/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.5967 - accuracy: 0.6667\n",
      "Epoch 19: val_loss did not improve from 0.61095\n",
      "2/2 [==============================] - 2s 832ms/step - loss: 0.5967 - accuracy: 0.6667 - val_loss: 0.6706 - val_accuracy: 0.5333\n",
      "Epoch 20/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6182 - accuracy: 0.6364\n",
      "Epoch 20: val_loss did not improve from 0.61095\n",
      "2/2 [==============================] - 2s 810ms/step - loss: 0.6182 - accuracy: 0.6364 - val_loss: 0.6231 - val_accuracy: 0.7333\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.6932 - accuracy: 0.5000\n",
      "Test accuracy: 50.0%\n"
     ]
    }
   ],
   "source": [
    "def get_sequence_model():\n",
    "    from tensorflow.keras.optimizers import Adam\n",
    "    class_vocab = label_processor.get_vocabulary()\n",
    "\n",
    "    frame_features_input = keras.Input((MAX_SEQ_LENGTH, NUM_FEATURES))\n",
    "    mask_input = keras.Input((MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "\n",
    "\n",
    "    x = keras.layers.GRU(16, return_sequences=True)(\n",
    "        frame_features_input, mask=mask_input\n",
    "    )\n",
    "    x = keras.layers.GRU(8)(x)\n",
    "    x = keras.layers.Dropout(0.5)(x)\n",
    "    x = keras.layers.Dense(8, activation=\"relu\")(x)\n",
    "    output = keras.layers.Dense(len(class_vocab), activation=\"softmax\")(x)\n",
    "\n",
    "    rnn_model = keras.Model([frame_features_input, mask_input], output)\n",
    "\n",
    "    rnn_model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return rnn_model\n",
    "\n",
    "\n",
    "def run_experiment():\n",
    "    filepath = \"/tmp/video_classifier\"\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "        filepath, save_weights_only=True, save_best_only=True, verbose=1\n",
    "    )\n",
    "\n",
    "    seq_model = get_sequence_model()\n",
    "    history = seq_model.fit(\n",
    "        [train_data[0], train_data[1]],\n",
    "        train_labels,\n",
    "        validation_split=0.3,\n",
    "        epochs=EPOCHS,\n",
    "        callbacks=[checkpoint],\n",
    "    )\n",
    "\n",
    "    seq_model.load_weights(filepath)\n",
    "    _, accuracy = seq_model.evaluate([test_data[0], test_data[1]], test_labels)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "\n",
    "    return history, seq_model\n",
    "\n",
    "\n",
    "_, sequence_model = run_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c3a58c",
   "metadata": {},
   "source": [
    "# Test del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15d8e64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_single_video(frames):\n",
    "    frames = frames[None, ...]\n",
    "    frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "    frame_features = np.zeros(shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\")\n",
    "\n",
    "    for i, batch in enumerate(frames):\n",
    "        video_length = batch.shape[0]\n",
    "        length = min(MAX_SEQ_LENGTH, video_length)\n",
    "        for j in range(length):\n",
    "            frame_features[i, j, :] = feature_extractor.predict(batch[None, j, :], verbose=0)\n",
    "        frame_mask[i, :length] = 1 \n",
    "\n",
    "    return frame_features, frame_mask\n",
    "\n",
    "\n",
    "def sequence_prediction(path):\n",
    "    class_vocab = label_processor.get_vocabulary()\n",
    "\n",
    "    frames = load_video(os.path.join(\"test\", path))\n",
    "    frame_features, frame_mask = prepare_single_video(frames)\n",
    "    probabilities = sequence_model.predict([frame_features, frame_mask], verbose=0)[0]\n",
    "\n",
    "    for i in np.argsort(probabilities)[::-1]:\n",
    "        print(f\"  {class_vocab[i]}: {probabilities[i] * 100:5.2f}%\")\n",
    "    return frames\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ebda0ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test video path: all_data/F_32_1_0_0_0_test.mp4\n",
      "  fight: 50.14%\n",
      "  normal: 49.86%\n"
     ]
    }
   ],
   "source": [
    "path = \"all_data/F_32_1_0_0_0_test.mp4\"\n",
    "print(f\"Test video path: {path}\")\n",
    "test_frames = sequence_prediction(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17131f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test video path: all_data/normal.mp4\n",
      "  fight: 50.14%\n",
      "  normal: 49.86%\n"
     ]
    }
   ],
   "source": [
    "path = \"all_data/normal.mp4\"\n",
    "print(f\"Test video path: {path}\")\n",
    "test_frames = sequence_prediction(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9cbc2ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test video path: all_data/normal_2.mp4\n",
      "  fight: 50.14%\n",
      "  normal: 49.86%\n"
     ]
    }
   ],
   "source": [
    "path = \"all_data/normal_2.mp4\"\n",
    "print(f\"Test video path: {path}\")\n",
    "test_frames = sequence_prediction(path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
